{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcj9LrYvdCL0"
   },
   "source": [
    "# Least Squares Regression\n",
    "\n",
    "Least Square regression is a determinstic model which means that unlike other stochastic model the output or the calculated weights does not depend on the state of the algorithm rather they solely depend on the input data.\n",
    "\n",
    "## Curve Fitting\n",
    "\n",
    "The method least square can be better understood by understanding the term curev fitting. In curve fitting, we try to \"fit\" a kind of curve onto a set of data points given to us as inputs. The curve can be of any kind for example a straight line, a quadratic or cubic curve and even a non standard curve.\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2020/10/Plot-of-Straight-Line-Fit-to-Economic-Dataset-1024x768.png\" width=\"500\">\n",
    "\n",
    "In the method of Least Square we try to fit a staright line onto the data points by minimizing the squared difference between the predicted value and the observed value of a given dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBrhTY_giCVx"
   },
   "source": [
    "# Math behind Least Square Regression (Multi-Linear)\n",
    "\n",
    "The model is fairly simple and it only relies on some basic matrix calculation (and also some partial derivatives but that won't be detailed in this notebook)\n",
    "\n",
    "## The Model\n",
    "\n",
    "To simplify the derivations we can omit the intercept term by centering the data as shown below:  \n",
    "\n",
    "\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1x_1 \\\\ \\bar{y} = \\beta_0 + \\beta_1\\bar{x} \\\\ y_i - \\bar{y} = 0 + \\beta_1 \\left( x_i - \\bar{x} \\right)$$  \n",
    "\n",
    "\n",
    "\n",
    "By using this fact we continue our analysis while omiting intercept.\n",
    "\n",
    "The general multi-linear model:\n",
    "\n",
    "\n",
    "\n",
    "$$y_i = \\beta_1 x_1 + \\beta_2 x_2 +\\dots+ \\beta_kx_k + \\epsilon_i$$  \n",
    "\n",
    "$$y_i = \\left [x_1,x_2,\\dots,x_k \\right ] \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} + \\epsilon_i$$  \n",
    "\n",
    "$$y_i = x^T \\beta + \\epsilon_i$$\n",
    "\n",
    "\n",
    "\n",
    "Now, We can write the above equation n times for the n observation in the data:    \n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{split}\\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots \\\\ y_n \\end{bmatrix} &=\n",
    "\\begin{bmatrix} x_{1,1} & x_{1,2} & \\ldots & x_{1,k}\\\\\n",
    "                x_{2,1} & x_{2,2} & \\ldots & x_{2,k}\\\\\n",
    "                \\vdots  & \\vdots  & \\vdots & \\vdots\\\\\n",
    "                x_{n,1} & x_{n,2} & \\ldots & x_{n,k}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} +\n",
    "\\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\\end{split}$$\n",
    "\n",
    "\n",
    "\n",
    "Now, Using matrix notation we can write the generated model as:  \n",
    "\n",
    "\n",
    "\n",
    "$$y = Xb + \\epsilon$$  \n",
    "\n",
    "Where:  \n",
    "\n",
    "y $\\rightarrow$ Matrix of predictions $(n\\times 1)$   \n",
    "\n",
    "X $\\rightarrow$ Matrix of feature values $(n\\times k)$  \n",
    "\n",
    "b $\\rightarrow$ Matrix of Coefficients $(n\\times 1)$  \n",
    "\n",
    "$\\epsilon \\rightarrow$ Matrix of error terms $(n\\times 1)$ \n",
    "\n",
    "## What is Least Squares in Least Squares:\n",
    "\n",
    "In a least squares model, we aim to minimize the sum of squares of the errors in vector(matrix) $\\epsilon$ . This least squares objective function can be written as:  \n",
    "\n",
    "\n",
    "\n",
    "$$f(b) = \\epsilon^T\\epsilon\\\\\\Rightarrow \\left ( y - Xb \\right)^T \\left( y - Xb \\right)\\\\ \\Rightarrow y^Ty - 2y^TXb+bX^TXb$$   \n",
    "\n",
    "\n",
    "\n",
    "By taking partial derivatives with respect to b and equate that to a vector of zeros to get the minimizing condition, We get:  \n",
    "\n",
    "\n",
    "\n",
    "$$b = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "\n",
    "\n",
    "And Bingo, This equation is the Crux of Least Square model. By using this equation we calculate all the required coefficients as long as the inverse of $(X^TX)$ exists.\n",
    "\n",
    "### Note: \n",
    "While implementing the model we just need to concatenate the data with a vector of one with shape $(n \\times 1)$. by doing this we force the model to calculate the intercept term.\n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} x_{1,1} & x_{1,2} & \\ldots & x_{1,k}\\\\\n",
    "                x_{2,1} & x_{2,2} & \\ldots & x_{2,k}\\\\\n",
    "                \\vdots  & \\vdots  & \\vdots & \\vdots\\\\\n",
    "                x_{n,1} & x_{n,2} & \\ldots & x_{n,k}\\\\\n",
    "\\end{bmatrix} \\Rightarrow \\begin{bmatrix} \n",
    "                1 & x_{1,1} & x_{1,2} & \\ldots & x_{1,k}\\\\\n",
    "                1 & x_{2,1} & x_{2,2} & \\ldots & x_{2,k}\\\\\n",
    "                \\vdots & \\vdots  & \\vdots  & \\vdots & \\vdots\\\\\n",
    "                1 & x_{n,1} & x_{n,2} & \\ldots & x_{n,k}\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVdDA6Bz3PcD"
   },
   "source": [
    "# Implementing Least Squares Regression from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uw2jGHDbsuHO",
    "outputId": "aa136141-81a2-4fa9-bbbc-433f2105d0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3853136 ,  0.1990597 , -0.60021688,  0.46210347,  0.06980208],\n",
       "       [ 0.13074058,  1.6324113 , -1.43014138, -1.24778318, -0.44004449],\n",
       "       [-0.77300978,  0.22409248,  0.0125924 , -0.40122047,  0.0976761 ],\n",
       "       [-0.57677133, -0.05023811, -0.23894805,  0.27045683, -0.90756366],\n",
       "       [-0.57581824,  0.6141667 ,  0.75750771, -0.2209696 , -0.53050115]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we need a sample dataset on which we can test our algorithms\n",
    "#Using sklearn to create a random regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X,y = make_regression(n_samples=200,\n",
    "                      n_features=5, \n",
    "                      n_targets=1, \n",
    "                      noise = 10,\n",
    "                      random_state=42)\n",
    "\n",
    "#looking at the generated Data\n",
    "X[:5,:] #First Five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3nDHE07y5cqa"
   },
   "outputs": [],
   "source": [
    "#Creating the Regression Algorithm\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "class MyLeastSquares():\n",
    "\n",
    "  def __init__(self):\n",
    "    self.coef = None #initializing a empty variable to store coefficients\n",
    "    self.intercept = None #initializing a empty variable to store intercept\n",
    "\n",
    "  def _concat_ones(self,X):\n",
    "    ones = np.ones(shape = X.shape[0]).reshape(-1,1) #Creating the method to concatenate ones\n",
    "    return np.concatenate((ones,X), axis = 1)\n",
    "\n",
    "  def fit(self,X,y): #Method to fit the model to a given data\n",
    "    if len(X.shape) == 1:\n",
    "      X = X.reshape(-1,1)\n",
    "    \n",
    "    X = self._concat_ones(X)\n",
    "    self.coef = inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "    self.intercept = self.coef[0]\n",
    "    self.coef = self.coef[1:]\n",
    "\n",
    "  def predict(self,x): #Method for prediction\n",
    "    if type(x) != 'numpy.ndarray': \n",
    "      x = np.array(x)\n",
    "\n",
    "    return self.intercept + x.dot(self.coef) #Calculate and return prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-66QDJ3qWVCA",
    "outputId": "f9228146-f522-45f9-a508-1f3966259cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Calculated Model:\n",
      "The Coefficients = [ 3.326 10.661 64.132 17.723 70.294]\n",
      "The Intercept 0.615\n"
     ]
    }
   ],
   "source": [
    "#Now, we can test our Linear Regression\n",
    "model = MyLeastSquares() #Creating an instances\n",
    "model.fit(X,y) #Fitting the model\n",
    "\n",
    "#Lets see the coefficients and the intercept\n",
    "print('The Calculated Model:')\n",
    "print('The Coefficients = {}'.format(model.coef.round(3)))\n",
    "print('The Intercept {:.3f}'.format(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIDqqCOGYzRm",
    "outputId": "6d053c5c-07f5-42d0-c8c3-4ef868bab7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Entry = [ 0.622 -0.562  0.632  0.708  0.973]\n",
      "The Acutal Value = 105.326\n",
      "The Predicted Value = 118.1528761494485\n"
     ]
    }
   ],
   "source": [
    "#Now, lets try to predict\n",
    "test = X[42,:] #taking the 42 entry from the data to predict\n",
    "print('Test Entry = {}'.format(test.round(3)))\n",
    "print('The Acutal Value = {}'.format(y.round(3)[42]))\n",
    "y_pred = model.predict(test)\n",
    "print('The Predicted Value = {}'.format(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "n-Hpm92ZakEu",
    "outputId": "014253d2-a9bc-47d6-e340-c7b3907073a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Error(Residual)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.842149</td>\n",
       "      <td>6.374</td>\n",
       "      <td>7.468149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-185.594177</td>\n",
       "      <td>-189.141</td>\n",
       "      <td>3.546823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-19.674477</td>\n",
       "      <td>-21.835</td>\n",
       "      <td>2.160523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.146519</td>\n",
       "      <td>-5.846</td>\n",
       "      <td>-0.300519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.596790</td>\n",
       "      <td>20.682</td>\n",
       "      <td>5.914790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41.386697</td>\n",
       "      <td>31.078</td>\n",
       "      <td>10.308697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-79.724308</td>\n",
       "      <td>-81.383</td>\n",
       "      <td>1.658692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-26.445052</td>\n",
       "      <td>-32.852</td>\n",
       "      <td>6.406948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-10.795434</td>\n",
       "      <td>-16.033</td>\n",
       "      <td>5.237566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64.733314</td>\n",
       "      <td>64.820</td>\n",
       "      <td>-0.086686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Predicted  Error(Residual)\n",
       "0   13.842149      6.374         7.468149\n",
       "1 -185.594177   -189.141         3.546823\n",
       "2  -19.674477    -21.835         2.160523\n",
       "3   -6.146519     -5.846        -0.300519\n",
       "4   26.596790     20.682         5.914790\n",
       "5   41.386697     31.078        10.308697\n",
       "6  -79.724308    -81.383         1.658692\n",
       "7  -26.445052    -32.852         6.406948\n",
       "8  -10.795434    -16.033         5.237566\n",
       "9   64.733314     64.820        -0.086686"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets see the predicted value for 10 randomly selected entries\n",
    "choices = np.random.choice(200, size=10, replace=False)\n",
    "batch_test = X[choices, :]\n",
    "y_actual = y[choices]\n",
    "y_actual = y_actual.reshape(-1,1)\n",
    "\n",
    "y_preds = []\n",
    "for test in batch_test:\n",
    "  y_preds.append(round(model.predict(test),3))\n",
    "\n",
    "#Making a dataframe of actual and predicted results\n",
    "y_preds = np.array(y_preds).reshape(-1,1)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.concatenate((y_actual,y_preds), axis = 1), columns = ['Actual', 'Predicted'])\n",
    "df['Error(Residual)'] = df['Actual'] - df['Predicted']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tke4t-kyZ70W"
   },
   "source": [
    "We have successfully implemented the Least Square Algorithm, we can also check that our algorithm works as it is supposed to by comparing the calculated model by our algorithm with that of calculated by sklearn Linear Regression Since sklearn Linear Regression is also a implementation of Least Square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDJX1KuBdJgf",
    "outputId": "069f043f-4607-4051-ed72-ea4aa350e41f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Using Sklearn:\n",
      "The Coefficients = [ 3.326 10.661 64.132 17.723 70.294]\n",
      "The Intercept 0.615\n"
     ]
    }
   ],
   "source": [
    "#Calculating Least Square model using sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X,y)\n",
    "\n",
    "#Lets see the coefficients and the intercept\n",
    "print('The Model Using Sklearn:')\n",
    "print('The Coefficients = {}'.format(model2.coef_.round(3)))\n",
    "print('The Intercept {:.3f}'.format(model2.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf9cPqrbdsDl"
   },
   "source": [
    "As, we can see these values are exactly same as calculated by our implementation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Understanding Least Square Regression",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
