{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLwPyeEnszXK"
   },
   "source": [
    "# Linear Regression Gradient Descent\n",
    "\n",
    "Gradient descent is a stochastic approach to minimize the error generated in a regression problem and therefore it is indeterministic in nature which means that in this method we try to approximate the solution rather than trying to find the exact closed form solution. Closed form solution being the least square method.\n",
    "\n",
    "## Why we use gradient descent when there is a closed form solution available(Least Squares)\n",
    "\n",
    "The answer is easy, Computational Efficiency.   \n",
    "In Least Squares method we use the given formula for calculating the coefficients or weights of the model:\n",
    "\n",
    "$$b = \\left( X^TX \\right)^{-1}X^Ty$$ \n",
    "\n",
    "A closer inspection reveals that for every solution we have to find we have to calculate $(X^TX)^{-1}$. Furthermore, calculating $X^TX$ is fine but calculating the 'Inverse of the given Matrix' is very computationally expensive.\n",
    "\n",
    "We know that for a problem in which there are $k$ independent variables the matrix $X^TX$ will have $k\\times k$ elements(For a mean centered approach). The operation that will invert the $k\\times k$ matrix has a complexity of $O(k^3)$.\n",
    "\n",
    "And that litle detail gives us the general case complexity for least sqaures which is $O(k^3)$. This is fine for smaller problems but as the dimensionality of the problem increases the time complexity starts becoming a problem.\n",
    "\n",
    "In a Gradient Descent approach, the method is linear in $k$ (Stochastic Gradient Descent) for a problem with dimensionality $k$. But we also need to iterate multiple times over the entire data set. which will add a constant to the complexity but regardless the gradient descent solution will always be faster than $O(K^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5d7Mxii0LTZ"
   },
   "source": [
    "# Math Behind Linear Regression Gradient Descent\n",
    "\n",
    "### The Basic Premise of the algorithm\n",
    "\n",
    "The approach is simple, \n",
    "\n",
    "1. Initialize the wights and intercept to $zero$. \n",
    "2. Use current model to make prediction.\n",
    "3. Calculate Error(Cost) from the prediction.\n",
    "4. Update the weights according to the Error.\n",
    "5. repeat steps 2 to 4 untill a statisfactory result is reached.\n",
    "\n",
    "\n",
    "Now, Implement the steps given we need to solve two key problems:\n",
    "\n",
    "1. How to setup the Cost function(That will evaluate and represent error/loss of the current model)\n",
    "2. How to update the weights.\n",
    "\n",
    "Lets, go through these one by one.\n",
    "\n",
    "### 1. Cost Function:\n",
    "\n",
    "A Cost function is nothing but a function that can calculate the error for the entire dataset. And we know that the error for a single prediction is calculated by:\n",
    "\n",
    "$$Error = y_{Actual} - y_{Predicted}$$\n",
    "\n",
    "$$Error = y - \\hat{y}$$\n",
    "\n",
    "$$Error = y - (mx+c)$$\n",
    "\n",
    "Now, for evaluating a cost function we can square the error to get rid of the negative sign and then sum all of the error for the n predictions\n",
    "\n",
    "$$Cost = \\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)^2$$\n",
    "\n",
    "Furthermore, Since the sum of all the error might get exceedingly large we can normalize the value by dividing it with the number of samples in the dataset which is n\n",
    "\n",
    "\n",
    "$$Cost = \\frac{1}{n}\\times \\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)^2$$\n",
    "\n",
    "$$Cost = \\frac{1}{n}\\times\\sum_{i=0}^{n} \\left( y_i - (mx_i +c) \\right)^2$$\n",
    "\n",
    "This is the cost function generally used in linear regression and it is called the Mean Squared Error or MSE.\n",
    "\n",
    "### 2. Updating Weights\n",
    "\n",
    "To Update weights we need to find two things: \n",
    "1. how much we need to change and \n",
    "2. do we need to increase or decrease(the direction of change)\n",
    "\n",
    "\n",
    "Both of these things can be extracted from the Gradient of the Cost Function.\n",
    "\n",
    "![](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png)\n",
    "\n",
    "\n",
    "The direction of decreasing Slope of the cost function will always points towards the minimum and the value of the slope itself can be used as a indication for the 'farness' of the minimum point. As, the slope is zero at the minimum and it increases as we go farther away from the minimum.\n",
    "\n",
    "\n",
    "So, we established that if we calculate the Gradient of the Cost Function we can find the direction as well as power by which we need to change the wieghts. Now, all is left is to calculate the gradient iteslf.\n",
    "\n",
    "First Lets Find the Gradient with respect to the weights:\n",
    "\n",
    "$$D_m = \\frac{\\partial (Cost~Function)}{\\partial m} = \\frac{\\partial}{\\partial m} \\left(\\frac{1}{n}\\times\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)^2\\right)$$\n",
    "\n",
    "\n",
    "$$D_m =\\frac{2}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\times \\frac{\\partial}{\\partial m} \\left( y_i - \\hat{y_i} \\right) \\right)$$\n",
    "\n",
    "$$D_m =\\frac{2}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\times \\frac{\\partial}{\\partial m} \\left( y_i - (mx_i + c) \\right) \\right)$$\n",
    "\n",
    "$$D_m =\\frac{2}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\times (-x_i) \\right)$$\n",
    "\n",
    "$$D_m =\\frac{-2}{n} \\left(\\sum_{i=0}^{n} x_i\\left( y_i - \\hat{y_i} \\right)\\right)$$\n",
    "\n",
    "\n",
    "Now, Calculating Gradient with respect to intercept:\n",
    "\n",
    "\n",
    "\n",
    "$$D_c = \\frac{\\partial (Cost~Function)}{\\partial c} = \\frac{\\partial}{\\partial c} \\left(\\frac{1}{n}\\times\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)^2\\right)$$\n",
    "\n",
    "$$D_c =\\frac{2}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\times \\frac{\\partial}{\\partial c} \\left( y_i - \\hat{y_i} \\right) \\right)$$\n",
    "\n",
    "$$D_c =\\frac{2}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\times \\frac{\\partial}{\\partial c} \\left( y_i - (mx_i + c) \\right) \\right)$$\n",
    "\n",
    "$$D_c =\\frac{2}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\times (-1) \\right)$$\n",
    "\n",
    "$$D_c =\\frac{-2}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\right)$$\n",
    "\n",
    "\n",
    "Ok, All the steps are done just one more thing we can omit the $2$ in both equation since a constant term does absolutely nothing as we will be implementing a learning rate in the algorithm.\n",
    "\n",
    "\n",
    "Now, the gradient become:\n",
    "\n",
    "$$D_m =-\\frac{1}{n} \\left(\\sum_{i=0}^{n} x_i\\left( y_i - \\hat{y_i} \\right)\\right)~~\\&~~D_c =-\\frac{1}{n} \\left(\\sum_{i=0}^{n} \\left( y_i - \\hat{y_i} \\right)\\right)$$\n",
    "\n",
    "Now, we will simply scale the update rule with a constant learning rate$(Lr)$. So, Finally our update rule will become:\n",
    "\n",
    "\n",
    "$$m = m - D_m\\times Lr$$\n",
    "$$c = c - D_c\\times Lr$$\n",
    "\n",
    "Done, This is the complete Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kER9LqqJNSMW"
   },
   "source": [
    "# Implementing The developed Gradient Descent Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWigl3Ng0J3_",
    "outputId": "7271220a-25be-4c79-bde4-4c21ee7f376c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3853136 ,  0.1990597 , -0.60021688,  0.46210347,  0.06980208],\n",
       "       [ 0.13074058,  1.6324113 , -1.43014138, -1.24778318, -0.44004449],\n",
       "       [-0.77300978,  0.22409248,  0.0125924 , -0.40122047,  0.0976761 ],\n",
       "       [-0.57677133, -0.05023811, -0.23894805,  0.27045683, -0.90756366],\n",
       "       [-0.57581824,  0.6141667 ,  0.75750771, -0.2209696 , -0.53050115]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we need a sample dataset on which we can test our algorithms\n",
    "#Using sklearn to create a random regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X,y = make_regression(n_samples=200,\n",
    "                      n_features=5, \n",
    "                      n_targets=1, \n",
    "                      noise = 10,\n",
    "                      random_state=42)\n",
    "\n",
    "#looking at the generated Data\n",
    "X[:5,:] #First Five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "00V-3fQ8zO8r"
   },
   "outputs": [],
   "source": [
    "#Creating the Gradient Descent Regression Algorithm\n",
    "import numpy as np\n",
    "\n",
    "class MyGradientDescentRegression():\n",
    "\n",
    "  def __init__(self, n_iterations=1000, learning_rate=0.001):\n",
    "\n",
    "    self.weights = None #Initializing weights vector\n",
    "    self.intercept = None #Initializing intercept\n",
    "    self.n_iterations = n_iterations #setting number of iterations\n",
    "    self.lr = learning_rate #setting learning rate\n",
    "\n",
    "  def _Gradient_Descent(self, n_samples, X, y_preds, y_act):\n",
    "    \n",
    "    #Compute Gradient\n",
    "    self._Dw = (-1) * (1/n_samples) * np.dot(X.transpose(), (y_act - y_preds))\n",
    "    self._Di = (-1) * (1/n_samples) * np.sum(y_act - y_preds)\n",
    "\n",
    "    #Update Model Parameters\n",
    "    self.weights = self.weights - (self.lr * self._Dw)\n",
    "    self.intercept = self.intercept - (self.lr * self._Di)\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    #Initializing Parameters\n",
    "    self.weights = np.zeros(n_features)\n",
    "    self.intercept = 0\n",
    "\n",
    "    for _ in range(self.n_iterations):\n",
    "\n",
    "      #Calculating Predictions\n",
    "      y_preds = self.intercept + X.dot(self.weights)\n",
    "\n",
    "      #Gradient Descent\n",
    "      self._Gradient_Descent(n_samples, X, y_preds, y)\n",
    "\n",
    "  def predict(self, x):\n",
    "    if type(x) != 'numpy.ndarray':\n",
    "      x = np.array(x)\n",
    "\n",
    "    return self.intercept + x.dot(self.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gt2xxQFg-TSn",
    "outputId": "871258f3-395a-4e8d-8668-a6d69bd203be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Calculated Model:\n",
      "The Coefficients = [ 3.3281 10.6609 64.1266 17.7224 70.2893]\n",
      "The Intercept 0.6163\n"
     ]
    }
   ],
   "source": [
    "#Now, we can test our Gradient Descent Algorithm\n",
    "model = MyGradientDescentRegression(n_iterations=1000, learning_rate=0.01) #Creating an instances\n",
    "model.fit(X,y) #Fitting the model\n",
    "\n",
    "#Lets see the weights and the intercept\n",
    "print('The Calculated Model:')\n",
    "print('The Coefficients = {}'.format(model.weights.round(4)))\n",
    "print('The Intercept {:.4f}'.format(model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vm_DrgSm-dcs",
    "outputId": "d21b4183-3192-412d-ac47-a1303265a62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Using Sklearn:\n",
      "The Coefficients = [ 3.3262 10.6607 64.1317 17.7234 70.2944]\n",
      "The Intercept 0.6146\n"
     ]
    }
   ],
   "source": [
    "#To judge the Algorithm we can compare the coefficients and intercept from sklearn linear regression\n",
    "#while coefficients won't be exatcly the same since sklearn uses a least square approach we should get comparable results\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X,y)\n",
    "\n",
    "print('The Model Using Sklearn:')\n",
    "print('The Coefficients = {}'.format(model2.coef_.round(4)))\n",
    "print('The Intercept {:.4f}'.format(model2.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBPBF3IFGcP9"
   },
   "source": [
    "The Model is quite similar, Hence we can use it as validation for our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "pvV-ATBV-f4F",
    "outputId": "7b4da537-9a5a-4b00-c305-075cd50331af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1a58a958-02cb-401b-acc7-f154a5f7d3f9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Error(Residual)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-34.009023</td>\n",
       "      <td>-31.890</td>\n",
       "      <td>-2.119023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.494266</td>\n",
       "      <td>13.710</td>\n",
       "      <td>5.784266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-166.181626</td>\n",
       "      <td>-152.174</td>\n",
       "      <td>-14.007626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-100.885604</td>\n",
       "      <td>-104.038</td>\n",
       "      <td>3.152396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.596790</td>\n",
       "      <td>20.684</td>\n",
       "      <td>5.912790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-18.468427</td>\n",
       "      <td>-19.859</td>\n",
       "      <td>1.390573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.073086</td>\n",
       "      <td>14.784</td>\n",
       "      <td>8.289086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.154295</td>\n",
       "      <td>3.702</td>\n",
       "      <td>3.452295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>111.386050</td>\n",
       "      <td>111.510</td>\n",
       "      <td>-0.123950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-42.317285</td>\n",
       "      <td>-46.296</td>\n",
       "      <td>3.978715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a58a958-02cb-401b-acc7-f154a5f7d3f9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1a58a958-02cb-401b-acc7-f154a5f7d3f9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1a58a958-02cb-401b-acc7-f154a5f7d3f9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       Actual  Predicted  Error(Residual)\n",
       "0  -34.009023    -31.890        -2.119023\n",
       "1   19.494266     13.710         5.784266\n",
       "2 -166.181626   -152.174       -14.007626\n",
       "3 -100.885604   -104.038         3.152396\n",
       "4   26.596790     20.684         5.912790\n",
       "5  -18.468427    -19.859         1.390573\n",
       "6   23.073086     14.784         8.289086\n",
       "7    7.154295      3.702         3.452295\n",
       "8  111.386050    111.510        -0.123950\n",
       "9  -42.317285    -46.296         3.978715"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets see the predicted value for 10 randomly selected entries\n",
    "choices = np.random.choice(200, size=10, replace=False)\n",
    "batch_test = X[choices, :]\n",
    "y_actual = y[choices]\n",
    "y_actual = y_actual.reshape(-1,1)\n",
    "\n",
    "y_preds = []\n",
    "for test in batch_test:\n",
    "  y_preds.append(round(model.predict(test),3))\n",
    "\n",
    "#Making a dataframe of actual and predicted results\n",
    "y_preds = np.array(y_preds).reshape(-1,1)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.concatenate((y_actual,y_preds), axis = 1), columns = ['Actual', 'Predicted'])\n",
    "df['Error(Residual)'] = df['Actual'] - df['Predicted']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear Regression Gradient Descent",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
